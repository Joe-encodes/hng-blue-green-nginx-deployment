# Blue/Green Deployment Runbook
# This runbook provides guidance for operators on understanding and responding to alerts
# generated by the Blue/Green deployment with observability.

## Alerts and Operator Actions

### üîÑ Failover Detected Alert

**What it means**: Traffic has automatically switched from one application pool (Blue or Green) to the other due to detected failures in the currently active pool.

**Operator Actions**:

1.  **Check Primary Pool Health**: Verify the health status of the pool that *was* active when the failover occurred.
    ```bash
    curl http://localhost:8081/healthz # For Blue app
    ```

2.  **Check Secondary Pool Health**: Ensure the pool that traffic *failed over to* is healthy and serving requests correctly.
    ```bash
    curl http://localhost:8082/healthz # For Green app
    ```

3.  **Investigate Primary Pool Issues**: Dive deeper into why the primary pool failed.
    -   **Check container logs**: Inspect the logs of the failed container for errors or unusual activity.
        Example: `docker logs ngnix_upstream-app_blue-1` (replace `app_blue` with `app_green` if Green failed)
    -   **Check resource usage**: Monitor CPU, memory, and network usage of the container.
        Example: `docker stats`
    -   **Verify chaos mode status**: If chaos testing is active, confirm it's the cause.
        Example: `curl http://localhost:8081/version` (check `X-App-Pool` and other headers)

4.  **Recovery**: Steps to take once the primary pool's issues are resolved.
    -   Fix identified issues in the primary pool's environment or code.
    -   If chaos was active, stop it: `curl -X POST http://localhost:8081/chaos/stop` (adjust port if Green was primary).
    -   Monitor for automatic failback (traffic returning to the original primary) or manually initiate a switch if needed.

### ‚ùå High Error Rate Alert

**What it means**: The percentage of 5xx errors (server-side errors) in requests has exceeded the configured `ERROR_RATE_THRESHOLD` over the last `WINDOW_SIZE` requests.

**Operator Actions**:

1.  **Check Current Error Rate**: Observe the Nginx access logs for patterns in errors.
    -   Monitor Nginx logs: `docker logs ngnix_upstream-nginx-1`
    -   Look for repeated 5xx status codes and their associated upstream addresses/pools.

2.  **Identify Affected Pool**: Determine which application pool (Blue or Green) is contributing to the high error rate.
    -   Analyze the `pool` field in Nginx logs (`X-App-Pool` header).

3.  **Investigate Application Issues**: Look into the specific application container experiencing errors.
    -   Check application logs: `docker logs ngnix_upstream-app_blue-1` or `ngnix_upstream-app_green-1`
    -   Verify resource constraints: Ensure the container has sufficient CPU/memory.
    -   Check dependencies: Are external services (databases, APIs) accessible and healthy?

4.  **Immediate Actions**:
    -   If the primary pool is causing the high error rate, confirm that failover to the backup pool has occurred or is in progress.
    -   Consider manual intervention (e.g., restarting the unhealthy container) if auto-failover is not effective or requires assistance.
    -   Check for cascading failures where one service's issue impacts others.

### ‚úÖ Recovery Alert (Optional / Inferred)

**What it means**: This alert signifies that a previously identified issue (failover or high error rate) has been resolved, and the system is operating normally. Often, recovery is inferred from the absence of further alerts and stable metrics.

**Operator Actions**:

1.  **Verify Primary Pool Health**: Confirm the health of the currently active primary pool.
    ```bash
    curl http://localhost:8080/version # Check X-App-Pool header
    ```
2.  **Monitor Nginx Logs**: Continuously observe logs to ensure traffic is flowing smoothly to the expected primary pool without errors.

## Troubleshooting Commands
# Essential Docker Compose commands for monitoring and debugging the deployment.

```bash
# Check the status of all services in the Docker Compose project
docker-compose ps

# View real-time Nginx access and error logs
docker-compose logs ngnix_upstream-nginx-1

# View logs for the Blue application service
docker-compose logs ngnix_upstream-app_blue-1

# View logs for the Green application service
docker-compose logs ngnix_upstream-app_green-1

# View logs for the Alert Watcher service
docker-compose logs ngnix_upstream-alert_watcher-1

# Test the main service endpoint via Nginx (should return Blue by default)
curl http://localhost:8080/version

# Test the Blue application directly (for health and version)
curl http://localhost:8081/healthz
curl http://localhost:8081/version

# Test the Green application directly (for health and version)
curl http://localhost:8082/healthz
curl http://localhost:8082/version

# Manual chaos testing: Start error mode on the Blue app to simulate failure
curl -X POST "http://localhost:8081/chaos/start?mode=error"

# Manual chaos testing: Stop chaos on the Blue app to restore functionality
curl -X POST "http://localhost:8081/chaos/stop"
```

## Alert Suppression (Maintenance Mode)
# Guidelines for temporarily disabling or adjusting alerts during planned maintenance or deployments.

During planned maintenance or controlled deployments, you might want to suppress alerts to avoid unnecessary notifications.

**Actions**:

-   **Temporarily stop the watcher**: This will halt all alerts. Remember to restart it after maintenance.
    ```bash
    docker-compose stop alert_watcher
    docker-compose start alert_watcher # To resume
    ```
-   **Modify thresholds**: Temporarily adjust `ERROR_RATE_THRESHOLD` or `ALERT_COOLDOWN_SEC` in your `.env` file before deployment, then restart `alert_watcher`.
    ```bash
    # Edit .env, e.g., set ERROR_RATE_THRESHOLD=100 for no errors
    docker-compose restart alert_watcher
    ```
-   **Utilize Slack's features**: Use Slack's "Do Not Disturb" mode or mute the alert channel during maintenance windows.
